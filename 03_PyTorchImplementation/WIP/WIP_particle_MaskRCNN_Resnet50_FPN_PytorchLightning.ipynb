{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchVision Object Detection Finetuning for IMPTOX Particles\n",
    "\n",
    "## Pytorch Lightning Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms.functional import to_tensor \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "\n",
    "\n",
    "#os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "#os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n",
    "\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#DEVICE = [0,1]\n",
    "#DEVICE = torch.device('cpu')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, resize=(256, 256)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the images and masks.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied on the target (mask).\n",
    "            class_mode (string): 'file' for different mask files per class, 'color' for different colors in a single mask.\n",
    "            color_mapping (dict): Mapping from color to class if class_mode is 'color'.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "\n",
    "        img_name = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        image = image.resize(self.resize, Image.BILINEAR)\n",
    "        \n",
    "        image = to_tensor(image).to(torch.float32)\n",
    "\n",
    "        # Get all masks for the current image (normally only one, but ready if multiple classes are present)\n",
    "        masks = []\n",
    "        for f in os.listdir(self.data_dir):\n",
    "            #print(f\"> current file: {f}\")\n",
    "            if f.startswith(self.image_files[idx].replace('.jpg', '')) and f.endswith('_mask.png'):\n",
    "                \n",
    "                mask_path = os.path.join(self.data_dir, f)\n",
    "                mask = Image.open(mask_path).convert('L')\n",
    "                \n",
    "                # Resize mask to a fixed size \n",
    "                mask = mask.resize((256, 256), Image.NEAREST)\n",
    "                \n",
    "                masks.append(np.array(mask))\n",
    "                \n",
    "        combined_mask = np.maximum.reduce(masks)\n",
    "        \n",
    "        #-----\n",
    "        from scipy.ndimage import label\n",
    "        # Use connected component labeling to find individual objects\n",
    "        labeled_array, num_features = label(combined_mask)\n",
    "        labeled_mask = torch.tensor(labeled_array)\n",
    "        \n",
    "        \n",
    "        #mask = torch.tensor(combined_mask, dtype=torch.uint8)\n",
    "        mask = torch.tensor(np.array(labeled_mask, dtype=np.uint8))\n",
    "        #print(f\"Mask: {mask.shape}\")\n",
    "\n",
    "        \n",
    "            \n",
    "        # Add the required information: boxes, labels, image_id, area, iscrowd and masks\n",
    "        \n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        \n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "        #print(f\"Masks: {masks}\")\n",
    "\n",
    "        # get bounding box coordinates for each mask. Clamp to avoid negative (invalid) values. \n",
    "        boxes = torch.clamp(masks_to_boxes(masks), min=0)\n",
    "        #print(f\"Boxes type: {type(boxes)}\")\n",
    "        \n",
    "        # We need to filter out boxes that have zero width or height\n",
    "        valid_boxes = []\n",
    "        for box in boxes:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            if (x_max > x_min) and (y_max > y_min):\n",
    "                valid_boxes.append(box)\n",
    "\n",
    "        # Convert back to tensor\n",
    "        boxes = torch.stack(valid_boxes) if valid_boxes else torch.empty((0, 4))\n",
    "        \n",
    "        \n",
    "        #print(f\"Boxes dimentions: {boxes.shape}\")\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(image)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks).to(DEVICE)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        \n",
    "        img = img.to(DEVICE)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, train_val_test_split=(0.7, 0.15, 0.15)):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_val_test_split = train_val_test_split\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "       # Define transforms\n",
    "        transform = T.Compose([\n",
    "            T.Resize((256, 256)),  # Resize images to 256x256\n",
    "            T.ToTensor(),\n",
    "            #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "        #target_transform = T.Compose([\n",
    "        #    T.Resize((256, 256)),  # Resize images to 256x256\n",
    "        #    T.ToTensor()\n",
    "        #    ])\n",
    "\n",
    "        \n",
    "        # Load dataset\n",
    "        self.dataset = CustomDataset(self.data_dir, transform=transform)#, target_transform=target_transform)\n",
    "        \n",
    "        # Split dataset into train, val, and test\n",
    "        # Calculate split sizes\n",
    "        train_size = int(self.train_val_test_split[0] * len(self.dataset))\n",
    "        val_size = int(self.train_val_test_split[1] * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size - val_size\n",
    "        self.train_data, self.val_data, self.test_data = random_split(self.dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, num_workers = NUM_WORKERS, pin_memory=True, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, num_workers = NUM_WORKERS, pin_memory=True, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, num_workers = NUM_WORKERS, pin_memory=True, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Instantiate the CustomDataModule\n",
    "# ----------------------------------\n",
    "\n",
    "data_dir = '/mnt/remote/workspaces/thibault.schowing/0_DATA/IMPTOX/00_Dataset/uFTIR_CurSquareSemantic.v1i.png-mask-semantic/train'\n",
    "batch_size = 4\n",
    "train_val_test_split = (0.6, 0.2, 0.2)\n",
    "\n",
    "\n",
    "\n",
    "dm = CustomDataModule(data_dir, batch_size = batch_size, train_val_test_split=train_val_test_split)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNModule(pl.LightningModule):\n",
    "    def __init__(self, num_classes, learning_rate=1e-3):\n",
    "        super(FasterRCNNModule, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # load a model pre-trained on COCO\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "        # replace the classifier with a new one, that has\n",
    "        # num_classes which is user-defined\n",
    "        num_classes = 2  # 1 class (person) + background\n",
    "       \n",
    "        \n",
    "        \n",
    "        # load an instance segmentation model pre-trained on COCO\n",
    "        self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "        # get number of input features for the classifier\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # now get the number of input features for the mask classifier\n",
    "        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        # and replace the mask predictor with a new one\n",
    "        self.model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "            in_features_mask,\n",
    "            hidden_layer,\n",
    "            num_classes\n",
    "        )\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        return self.model(x, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
