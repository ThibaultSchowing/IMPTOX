{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790404c5-c3a3-4c7e-ac62-12431ea0061b",
   "metadata": {},
   "source": [
    "# Create the ultimate neural network by using only curated data\n",
    "\n",
    "# -> version with square images at the end\n",
    "\n",
    "With images manually annotated and curated, from saturated and clean filters, we train an ultimate neural network. \n",
    "The curated data are augmented to 500 images as much as the free plan allows. Additional augmentation is not necessary as the saturated filters annotations are not based on highly recognizable features (human biased - perception - )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e77dd-7ecd-456e-980b-3f5ea103d0f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Remove old data before downloading another version of the dataset. \n",
    "\n",
    "```! rm -r ../../../0_DATA/IMPTOX/01-01_image_library/V4/download/*```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e8d9a-e570-4231-9cde-592b870d129c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! rm -r ../../../0_DATA/IMPTOX/01-01_image_library/V4/download/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaaa84a-9f44-4eed-a6a9-1eae2c92f61a",
   "metadata": {},
   "source": [
    "Use roboflow SDK to login and download the chosen version of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8f773-53f2-4058-a9f7-54f97dfd9c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#V6\n",
    "import roboflow \n",
    "\n",
    "roboflow.login()\n",
    "rf = roboflow.Roboflow()\n",
    "\n",
    "#rf = Roboflow(api_key=\"...\")\n",
    "project = rf.workspace(\"uftir-particles\").project(\"uftir_curated\")\n",
    "download_folder = \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/\"\n",
    "\n",
    "# Uncomment to download. It will overwrite the json annotation files !! \n",
    "\n",
    "new_dataset = False\n",
    "\n",
    "if new_dataset: \n",
    "    # Download and process the data again only if they have changed. \n",
    "    dataset = project.version(6).download(model_format=\"coco\", location=download_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a601ae-0288-4dbe-b31d-b71be73fbaf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#V7\n",
    "import roboflow \n",
    "\n",
    "roboflow.login()\n",
    "rf = roboflow.Roboflow()\n",
    "\n",
    "#rf = Roboflow(api_key=\"...\")\n",
    "project = rf.workspace(\"uftir-particles\").project(\"uftir_curated\")\n",
    "download_folder = \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/\"\n",
    "\n",
    "# Uncomment to download. It will overwrite the json annotation files !! \n",
    "\n",
    "new_dataset = False\n",
    "\n",
    "if new_dataset: \n",
    "    # Download and process the data again only if they have changed. \n",
    "    dataset = project.version(7).download(model_format=\"coco\", location=download_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe92128-9652-4d0c-bb87-fb617ea67449",
   "metadata": {},
   "source": [
    "The particle object is present twice (god knows why). We must remove it once for every annotation. \n",
    "\n",
    "Modification of all three annotation files (train, test, validation), keep only the following:  \n",
    "\n",
    "\n",
    "```\n",
    " \"categories\": [{\n",
    "            \"id\": 0,\n",
    "            \"name\": \"particle\",\n",
    "            \"supercategory\": \"particles\"\n",
    "        }\n",
    "    ],\n",
    "```\n",
    "\n",
    "Then replace all the \"\"category_id\": 1,\" by \"\"category_id\": 0,\" if you correctly changed the ids as over. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa55db-ddb2-4d42-8012-e9c864063e07",
   "metadata": {},
   "source": [
    "## Register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6435244-fd34-4ea2-8b4a-667c5457cd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027de5de-cc31-4edc-95b9-9a0a09a76701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "\n",
    "\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DatasetCatalog.clear()\n",
    "MetadataCatalog.clear()\n",
    "\n",
    "# Register the train, test, and validation datasets\n",
    "register_coco_instances(\"train_particles_v6\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/train/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/train\")\n",
    "register_coco_instances(\"test_particles_v6\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/test/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/test\")\n",
    "register_coco_instances(\"val_particles_v6\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/valid/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v6/valid\")\n",
    "\n",
    "# Define metadata for your classes\n",
    "MetadataCatalog.get(\"train_particles_v6\").set(thing_classes=[\"particle\"])\n",
    "MetadataCatalog.get(\"test_particles_v6\").set(thing_classes=[\"particle\"])\n",
    "MetadataCatalog.get(\"val_particles_v6\").set(thing_classes=[\"particle\"])\n",
    "\n",
    "\n",
    "# V7 with modified augmentation parameters\n",
    "\n",
    "# Register the train, test, and validation datasets\n",
    "register_coco_instances(\"train_particles_v7\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/train/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/train\")\n",
    "register_coco_instances(\"test_particles_v7\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/test/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/test\")\n",
    "register_coco_instances(\"val_particles_v7\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/valid/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V4/download_v7/valid\")\n",
    "\n",
    "# Define metadata for your classes\n",
    "MetadataCatalog.get(\"train_particles_v7\").set(thing_classes=[\"particle\"])\n",
    "MetadataCatalog.get(\"test_particles_v7\").set(thing_classes=[\"particle\"])\n",
    "MetadataCatalog.get(\"val_particles_v7\").set(thing_classes=[\"particle\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For the saturated filters (additional - augemented from V3) \n",
    "\n",
    "# Register the train, test, and validation datasets\n",
    "register_coco_instances(\"test_sat_particles\", {}, \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V3/download_version2_augmented/train/_annotations.coco.json\", \n",
    "                        \"../../../0_DATA/IMPTOX/01-01_image_library/V3/download_version2_augmented/train\")\n",
    "\n",
    "# Define metadata for your classes\n",
    "MetadataCatalog.get(\"test_sat_particles\").set(thing_classes=[\"particle\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae759044-7bd3-4aa3-951e-2df518b07628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5582b67-b6df-40d2-a073-9fcc352032b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize datasets\n",
    "\n",
    "1. Train\n",
    "2. Val\n",
    "3. Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a9ab1-7ba8-423d-8583-b54d8eff67c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mosaic(dataset_name, max_images=30):\n",
    "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "    \n",
    "    num_images = min(len(dataset_dicts), max_images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 5))\n",
    "    \n",
    "    for i, d in enumerate(random.sample(dataset_dicts, num_images)):\n",
    "        image = plt.imread(d[\"file_name\"])\n",
    "        v = Visualizer(image, metadata=metadata, scale=0.8)\n",
    "        v = v.draw_dataset_dict(d)\n",
    "        axes[i].imshow(v.get_image()[:, :, ::-1])\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Image {i+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0da17-aade-4375-aa61-e95d47b3ec70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_mosaic(\"train_particles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361c9f9-72f1-4c48-be1f-b23e2aa22fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_mosaic(\"val_particles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e3a5a-ea68-4397-b390-a50c8c35daff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_mosaic(\"test_particles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85284614-15ce-444a-8964-2da6ad21e725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_mosaic(\"test_sat_particles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5672a-37c4-4e15-b037-86b567e7da89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure and train detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a3e4e-6fd3-448b-aa96-d5eb31bcf89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.OUTPUT_DIR = \"./TrainDetectron2Model\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "cfg.merge_from_file(\n",
    "    \"../../Other/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    #\"../../Other/detectron2/configs/Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml\"\n",
    ") # Detectron2 accidentally installed at more than one place\n",
    "\n",
    "# Specify the train, test, and validation subsets in your Detectron2 configuration\n",
    "cfg.DATASETS.TRAIN = (\"train_particles\",)\n",
    "cfg.DATASETS.TEST = (\"test_particles\",)  # Possible to later change the test set\n",
    "cfg.DATASETS.VAL = (\"val_particles\",) # You can add validation dataset here if you want to evaluate during training\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "\n",
    "# Before training\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
    "cfg.MODEL.DEVICE = 'cuda:1'  # cpu or cuda:0-1\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.005\n",
    "cfg.SOLVER.MAX_ITER = (\n",
    "    300\n",
    ")  # 300 iterations seems good enough, but you can certainly train longer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (\n",
    "    512\n",
    ")  # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 class: particle\n",
    "\n",
    "\n",
    "# Set the number of augmented images per iteration\n",
    "cfg.INPUT.AUG.AUGMENTATIONS_PER_BATCH = 6  # Adjust this value as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca209a84-8b84-4c83-a591-8505dfb6f7b7",
   "metadata": {},
   "source": [
    "Train if needed, otherwise load the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8dde8-f683-4b37-8a88-5df4c3f49d11",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#new_parameters = True\n",
    "\n",
    "#if new_parameters:\n",
    "    \n",
    "# If the parameters have changed, retrain the neural net. Otherwise, just load the weights. \n",
    "\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9d0d0-cff6-4ef6-9d15-347b1d155995",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb99c25-017f-4d72-a04f-068391bc6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.01   # set the testing threshold for this model\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 2000\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0a4a3-5e5b-48c4-a76c-9f3a7cce6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation dataset\n",
    "val_dataset_name = \"test_particles\"\n",
    "val_dataset_dicts = DatasetCatalog.get(val_dataset_name)\n",
    "\n",
    "# Prepare a table for plotting\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "for d in val_dataset_dicts:\n",
    "    image_path = d[\"file_name\"]\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    predictions.append(instances)\n",
    "    ground_truths.append(d)  # Append the whole dictionary, containing \"file_name\" and \"annotations\"\n",
    "\n",
    "    \n",
    "evaluator = COCOEvaluator(val_dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, val_dataset_name)\n",
    "\n",
    "# Perform evaluation using the predictor and evaluator\n",
    "metrics = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9685db5e-9344-4f6f-b896-db3f46f3f655",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Plot a few graphics to compare predictions with ground truth\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    image_path = ground_truths[i][\"file_name\"]\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    v_pred = Visualizer(image[:, :, ::-1], metadata=MetadataCatalog.get(val_dataset_name), scale=0.8)\n",
    "    v_pred = v_pred.draw_instance_predictions(predictions[i])\n",
    "\n",
    "    v_gt = Visualizer(image[:, :, ::-1], metadata=MetadataCatalog.get(val_dataset_name), scale=0.8)\n",
    "    v_gt = v_gt.draw_dataset_dict(ground_truths[i])\n",
    "\n",
    "    # Plot the predicted and ground truth instances side by side\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(v_pred.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(v_gt.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a5a8e-8c14-478b-8d1e-e13160784774",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test with testset\n",
    "\n",
    "Test the neural network with the test dataset (or validation) as the test set does not contain saturated filters. \n",
    "\n",
    "Results are approx. 18 APs when using the validation sets (many images from both types) and 25 when using the test set that contains clean filters only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bf80c-fd53-4e08-8b49-7a9a8ebb2a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23ff8e18-20e7-4c62-82b5-4fd5e3201775",
   "metadata": {},
   "source": [
    "### Test with the satureated only dataset\n",
    "\n",
    "Here we use the unaugmented set of saturated particles (V3) to test the performances of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0638a12-7cc2-4d2b-905b-b81860d4fef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the evaluation dataset\n",
    "val_dataset_name = \"test_sat_particles\"\n",
    "val_dataset_dicts = DatasetCatalog.get(val_dataset_name)\n",
    "\n",
    "# Prepare a table for plotting\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "for d in val_dataset_dicts:\n",
    "    image_path = d[\"file_name\"]\n",
    "    image = cv2.imread(image_path)\n",
    "    outputs = predictor(image)\n",
    "\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    predictions.append(instances)\n",
    "    ground_truths.append(d)  # Append the whole dictionary, containing \"file_name\" and \"annotations\"\n",
    "\n",
    "    \n",
    "evaluator = COCOEvaluator(val_dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, val_dataset_name)\n",
    "\n",
    "# Perform evaluation using the predictor and evaluator\n",
    "metrics = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135207d-72b4-496f-89a7-82d2ece41946",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Plot a few graphics to compare predictions with ground truth\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    image_path = ground_truths[i][\"file_name\"]\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    v_pred = Visualizer(image[:, :, ::-1], metadata=MetadataCatalog.get(val_dataset_name), scale=0.8)\n",
    "    v_pred = v_pred.draw_instance_predictions(predictions[i])\n",
    "\n",
    "    v_gt = Visualizer(image[:, :, ::-1], metadata=MetadataCatalog.get(val_dataset_name), scale=0.8)\n",
    "    v_gt = v_gt.draw_dataset_dict(ground_truths[i])\n",
    "\n",
    "    # Plot the predicted and ground truth instances side by side\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(v_pred.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(v_gt.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c6c38-8273-4076-bbff-9b8622d7f7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced0d38-7094-4e4b-b17f-89896180f59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975b426-7884-4d5c-be71-15796934059a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7144b11f-9100-4a05-b468-810e22dcce90",
   "metadata": {},
   "source": [
    "The APs of around 11 indicates that the model performs poorly on saturated filters. However, the subjectivity of the detection and the overall purpose to scan almost random points in order to avoid scanning the whole filter makes this operation useful anyway. Indeed, the primary goal is to have a selection of points on the filter instead of a random detection like with the first version of the neural network, only working with clean filters. Secondly, Gepard, the end user of this NN, allows for manual correction and max size selection, rendering the small mistakes and inconsistencies of this neural network secondary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be95777-8f88-45c8-b61e-b81c4291e577",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "To assess the quality of the NN, let's compare the predicted data with the real ones. The metrics given by Detectron2 are the followin: \n",
    "\n",
    "\n",
    "1. AP (Average Precision):\n",
    "\n",
    "    AP stands for Average Precision. It is a common metric used to evaluate the accuracy of object detection models. AP calculates the precision-recall curve for different confidence thresholds and then computes the area under that curve. AP takes into account how well the model detects objects at various confidence thresholds.\n",
    "\n",
    "2. AP50 (Average Precision at 50% IoU):\n",
    "\n",
    "    AP50 is the Average Precision computed at an Intersection over Union (IoU) threshold of 0.5. It evaluates how well the model performs when the predicted bounding boxes have an overlap of at least 50% with the ground truth bounding boxes.\n",
    "\n",
    "3. AP75 (Average Precision at 75% IoU):\n",
    "\n",
    "    AP75 is the Average Precision computed at an IoU threshold of 0.75. It measures the model's accuracy when the predicted bounding boxes have an overlap of at least 75% with the ground truth bounding boxes.\n",
    "\n",
    "4. **APs (Average Precision for Small objects):**\n",
    "\n",
    "    APs is the Average Precision calculated specifically for small objects. It evaluates the model's performance on small-sized objects in the images.\n",
    "\n",
    "5. APm (Average Precision for Medium objects):\n",
    "\n",
    "    APm is the Average Precision calculated specifically for medium-sized objects. It evaluates the model's performance on objects of medium size in the images.\n",
    "\n",
    "6. APl (Average Precision for Large objects):\n",
    "\n",
    "    APl is the Average Precision calculated specifically for large-sized objects. It evaluates the model's performance on large objects in the images.\n",
    "\n",
    "In summary, these scores provide insights into the object detection model's performance across different aspects, such as overall AP, performance at different IoU thresholds, and performance on objects of different sizes. The higher the values, the better the model's performance for each of these metrics.\n",
    "\n",
    "APs (Average Precision for Small objects) focuses on measuring how well the model detects and localizes small objects, which are generally more challenging to detect due to their limited spatial extent and lower visual prominence compared to larger objects.\n",
    "\n",
    "To compute APs, the model's predictions for small objects are compared to the ground truth bounding boxes of small objects in the dataset. The evaluation is done based on an Intersection over Union (IoU) threshold (usually 0.5) to determine if a predicted bounding box is a true positive or a false positive.\n",
    "\n",
    "A comparison of good and bad scores for APs:\n",
    "\n",
    "Good APs Score:\n",
    "A good APs score would be close to 1.0 (or 100 in our case), indicating that the model is very effective at detecting and localizing small objects. A score close to 1.0 means that there is a high precision-recall trade-off, implying that the model successfully finds most small objects with very few false positives.\n",
    "\n",
    "Bad APs Score:\n",
    "A bad APs score would be close to 0.0, indicating poor performance in detecting and localizing small objects. A score close to 0.0 means that the model is not effective at identifying small objects accurately, leading to numerous false positives or missing many small objects, or both.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef888b9d-320e-47b4-a90f-23386acecee8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analysis\n",
    "\n",
    "The APs of 18.4 is relatively low\n",
    "\n",
    "More in-depth analysis and fine tuning in next notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74465195-1a65-44a0-b2b0-818bda6742f3",
   "metadata": {},
   "source": [
    "# Square 256x256 images\n",
    "\n",
    "Train a new NN with square images \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a997c-4e4b-493a-88ef-b3561eecb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.OUTPUT_DIR = \"./TrainDetectron2ModelSquare\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "cfg.merge_from_file(\n",
    "    \"../../Other/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    #\"../../Other/detectron2/configs/Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml\"\n",
    ") # Detectron2 accidentally installed at more than one place\n",
    "\n",
    "# Specify the train, test, and validation subsets in your Detectron2 configuration\n",
    "cfg.DATASETS.TRAIN = (\"train_particles\",)\n",
    "cfg.DATASETS.TEST = (\"test_particles\",)  # Possible to later change the test set\n",
    "cfg.DATASETS.VAL = (\"val_particles\",) # You can add validation dataset here if you want to evaluate during training\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "\n",
    "# Before training\n",
    "cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
    "cfg.MODEL.DEVICE = 'cuda:0-1'  # cpu or cuda:0-1\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.005\n",
    "cfg.SOLVER.MAX_ITER = (\n",
    "    300\n",
    ")  # 300 iterations seems good enough, but you can certainly train longer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (\n",
    "    512\n",
    ")  # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 class: particle\n",
    "\n",
    "\n",
    "# Set the number of augmented images per iteration\n",
    "cfg.INPUT.AUG.AUGMENTATIONS_PER_BATCH = 6  # Adjust this value as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680b9a8-9133-425e-9070-b1bc4fe79ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1306cd-d649-48de-8fd4-a7f727289b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648eb94-8040-4ea8-9d95-23e782b944e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca881622-29ba-43e2-9d87-7da42cb164b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb16a1-0513-4130-b21c-148e7041003f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4738ec2-7c42-437e-8102-74bead33c31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40658ee2-7573-43cd-8f12-d7663fbb180a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6dcdd-9d36-4a47-a289-efc3d8dcddaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496327c-249e-4612-ace7-1e90748876b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49621a-e043-4866-9d87-2390fa98e728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce2a37-c012-4e14-9d4d-8ca7ca058dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e227374-f450-43c5-925e-6d586bc8fbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a8cb1-c37b-4881-bc6a-826b1c50af95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda85f3-c6f9-4db1-b2c8-e86a92e37dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66659882-1236-42b5-a7db-b3c6a21de526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1dceb9-b25a-47c0-bb37-e2867e7f6efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9005cc-0f5d-4e70-928e-76139e6dbaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efd826e-8e7d-4f5f-9c73-43abb86d857a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv-kernel",
   "language": "python",
   "name": "aienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
